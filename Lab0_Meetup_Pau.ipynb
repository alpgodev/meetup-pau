{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<small><i>Ce notebook a été créé par Yann Vernaz (2017).</i></small>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<a href=\"https://www.meetup.com/fr-FR/Meetup-Machine-Learning-Pau/\" ><img src=\"img/meetup_logo.png\" style=\"float:left; max-width: 100px; display: inline\" alt=\"Meetup\"/></a> \n",
    "<a href=\"https://www.meetup.com/fr-FR/Meetup-Machine-Learning-Pau/\" ><img src=\"img/meetup_ML_pau.png\" style=\"float:center; max-width: 250px; display: inline\"  alt=\"Meetup Machine Learning Pau\"/></a>\n",
    "<a href=\"http://www.helioparc.com\" ><img src=\"img/helioparc_logo.svg\" style=\"float:right; max-width: 200px; display: inline\" alt=\"Technopole Héloparc\"/> </a>\n",
    "</center>\n",
    "<br>\n",
    "<hr>\n",
    "<center><h1>Optimisation distribuée avec Apache Spark</h1></center>\n",
    "<hr>\n",
    "<center><h2>Lab 0 - Test de l'image Docker</h2></center>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Dans ce _Notebook_ nous allons tester que notre image _Docker_ fonctionne correctement. Nous allons vérifier que l'on peut créer l'environnement _Spark_ et ensuite explorer les données fournies pour les travaux pratiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1>Création de l'environnement</h1>\n",
    "<br>\n",
    "<a href=\"http://spark.apache.org\" ><img src=\"img/spark_logo.png\" style=\"float:center; max-width: 320px; display: inline\" alt=\"Apache Spark\"/></a>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Rappel**\n",
    ">Dans un _Notebook Jupyter_, il faut appuyez sur `[Ctrl-Enter]` pour exécuter le contenu d'une cellule ou appuyer sur le bouton `run cell` dans la barre des outils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PySpark version:2.2.0\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "\n",
    "# using Spark local mode set to # cores on your machine\n",
    "conf = SparkConf()\n",
    "conf.setMaster(\"local[*]\")\n",
    "conf.setAppName(\"Meetup Machine Learning Pau\")\n",
    "\n",
    "sc = SparkContext(conf=conf)\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "print(\"PySpark version:\" + str(sc.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "L'interface **Spark UI** est alors accessible dans votre navigateur web à partir de l'adresse http://localhost:4040. Cette interface affiche des informations utiles sur l'application (on en discutera lors des travaux pratiques).\n",
    "\n",
    "- Une liste des étapes et des tâches du planificateur.\n",
    "- Informations sur l'utilisation de la mémoire (taille des `RDD`, ...).\n",
    "- Informations sur l'environnement.\n",
    "- Informations sur les exécuteurs/noeuds (_workers_) en cours d'exécution.\n",
    "- ...\n",
    "\n",
    "Note - Cette interface permet de réaliser un _monitoring_ à minima de notre session _Spark_ en cours d'exécution. Plusieurs outils externes peuvent être également utilisés pour profiler les performances de _Spark_, par exemple [Ganglia](http://ganglia.sourceforge.net).\n",
    "\n",
    "Normalement vous devez voir ceci dans votre navigateur :\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<a href=\"http://spark.apache.org/docs/latest/monitoring.html\" ><img src=\"img/screen_shot_Spark_UI.png\" style=\"float:center; display: inline\" alt=\"Apache Spark UI\"/></a>\n",
    "</center>\n",
    "\n",
    "Maintenant vous pouvez utiliser _Spark_ en mode _local_ (sur votre ordinateur). Dans la suite, nous allons découvrir les données que nous utiliserons lors des travaux pratiques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>\n",
    "<h1>Les données</h1>\n",
    "<br>\n",
    "</center>\n",
    "<p>Les données (fichier [retail.csv](retail.csv)) sont issues du programme de fidélité d'une grande chaîne de distribution alimentaire. Ce programme (_i.e._ carte de fidélité) permet de collecter des données sur les transactions des clients. Seule une petite fraction des données sont fournies (8 Mo sur plusieurs centaines de To). L'objectif est de prédire la probabilité du risque de désabonnement et ainsi détecter au plus tôt les clients succeptibles de quitter l'enseigne. Les données sont labéllisées (on connait les clients infidèles) et lors des travaux pratiques nous mettrons en oeuvre une méthode de classification supervisée simple (la régression logistique).</p>\n",
    "\n",
    "Chaque ligne représente un client et chaque colonne contient les attributs suivants:\n",
    "\n",
    "| Variable  | Description  |  Example |\n",
    "|:--------------------- |:-------------- | ---------- |\n",
    "| _clientId_  | Identifiant unique du client. | 901000010282532503 |\n",
    "| _meanAcquiredPts_  | Nombre moyen de points acquis. | 0.40 |\n",
    "| _meanConvertedPts_ | Nombre moyen de points convertis. | 0.0 |\n",
    "| _relationLength_   | Durée de la relation (en nombre de jours). | 638 |\n",
    "| _stdFreq_          | Écart-type de la fréquence des achats (en jours). | 47.60 |\n",
    "| _meanArticle_      | Nombre d'articles moyen achetés à chaque visite. | 12.33 |\n",
    "| _meanAmount_       | Panier moyen (en Euros). | 33.78 |\n",
    "| _nbVisits_         | Nombre de visites depuis le début de la relation. | 24 |\n",
    "| _meanFreq_         | Fréquence moyenne des achats (en jours). | 26.58 |\n",
    "| _lastTime_         | Temps écoulé depuis la dernière visite. | 33|\n",
    "| _stdAmount_        | Écart-type sur le montant du panier (en Euros). | 21.36 | \n",
    "| _stdArticle_       | Écart-type sur le nombre d'articles dans le panier. | 7.98 |\n",
    "    \n",
    "La variable cible (_label_) est donnée par _churn?_ qui vaut $\\color{red}{1}$ si le client a quitté l'enseigne et $\\color{blue}{0}$ sinon."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Chargement des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le fichier de données retail.csv existe.\n",
      "Vous pouvez continuer.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "\n",
    "inputFile = \"retail.csv\"\n",
    "\n",
    "if os.path.isfile(inputFile) != True:\n",
    "    print(\"Le fichier de données \"+ inputFile + \" n'existe pas.\")\n",
    "else:\n",
    "    print(\"Le fichier de données \" + inputFile + \" existe.\")\n",
    "    print(\"Vous pouvez continuer.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "retailRDD = sc.textFile(inputFile).map(lambda line: line.split(','))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Remarque** - Cette ligne de code crée une variable `retailRDD` (techniquement un `RDD`) qui pointe vers le fichier de données. La nature paresseuse (i.e. _lazy_) de `Spark` signifie qu'il n'exécute pas le code. Il attend une _action_ qui nécessite un calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nous supprimons l'en-tête du fichier\n",
    "header = retailRDD.first()\n",
    "retailRDD = retailRDD.filter(lambda line: line != header)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On vient de réaliser une _action_ (une exécution) en utilisant la fonction `.first()` qui récupère le premier élément du `RDD`, ici l'en-tête du fichier. Le `.filter()`comme le `.map()` sont des _transformations_ qui n'exécutent pas de code. \n",
    "\n",
    "Si vous regardez l'interface **Spark UI** vous devez voir ceci (ce qui correspond à l'action `.first()`) :\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"img/screen_shot_Spark_UI_2.png\" style=\"float:center; display: inline\" alt=\"Apache Spark UI\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comptons le nombre d'exemples, de variables (ou _features_) et la proportion de la classe \"1\" (clients qui ont quitté l'enseigne)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le jeux de données contient 57930 clients et a 12 attributs. \n",
      "Il y a 4.87% des clients qui ont quittés l'enseigne.\n"
     ]
    }
   ],
   "source": [
    "numExamples = retailRDD.count()\n",
    "numFeatures = len(retailRDD.take(1)[0])-1\n",
    "numClass1   = retailRDD.map(lambda row: row[0]==\"1\").sum()\n",
    "numClass0   = retailRDD.map(lambda row: row[0]==\"0\").sum()\n",
    "\n",
    "print(\"Le jeux de données contient {} clients et a {} attributs. \\\n",
    "\\nIl y a {:.2f}% des clients qui ont quittés l'enseigne.\" \\\n",
    "      .format(numExamples, numFeatures, numClass1/float(numExamples)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-traitements\n",
    "\n",
    "Dans un premier temps nous allons transformer notre `RDD` pour pouvoir utiliser le format <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.mllib.html#pyspark.mllib.regression.LabeledPoint\">`LabeledPoint`</a> qui est plus simple à manipuler. Un `LabeledPoint` se  compose d'un _label_ et d'un vecteur de _features_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "# On transforme notre RDD de tuple en un RDD de LabelPoint.\n",
    "# Par le même occasion on retire clientID des variables.\n",
    "labelPointRDD = retailRDD.map(lambda line: LabeledPoint(line[0],[line[2:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [1.30818181818,0.0,595.0,81.4560673559,36.0,114.705454545,11.0,54.0909090909,10.0,103.729869022,33.9352324288])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelPointRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons à présent un `RDD` dont chaque élément est un `LabelPoint` qui contient le _label_ et le vecteur des 11 _features_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La seconde étape (importante) de l'apprentissage par régression consiste à :\n",
    "\n",
    "* Ajouter un _intercept_, c'est une variable supplémentaire égale à $1$.\n",
    "\n",
    "* Normaliser les données pour avoir une moyenne nulle et une variance unitaire pour chaque variable (sauf l'_intercept_). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "import numpy as np\n",
    "\n",
    "# mean vector\n",
    "mean = labelPointRDD.map(lambda row: row.features.toArray()).reduce(add)/numExamples\n",
    "\n",
    "# std\n",
    "std = np.sqrt(labelPointRDD.map(lambda row: np.power(row.features.toArray()-mean, 2)).reduce(add))\n",
    "\n",
    "# scaled features\n",
    "data_scaled = labelPointRDD.map(lambda row: LabeledPoint(row.label, np.append((row.features.toArray()-mean)/std, 1.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [-0.000414035031732,-0.0024740538601,-0.00341258853829,0.0124919073001,0.00520893750325,0.00738817026779,-0.00443563405927,0.0186510548985,-0.00103584854704,0.0131807431244,0.0109816170485,1.0])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_scaled.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour terminer nous allons séparer notre jeux de données en deux parties : \n",
    "\n",
    "* Un ensemble de données d'entraînement sur lequel nous estimerons le modèle ($70\\%$).\n",
    "* Un ensemble de données de test sur lequel nous testerons nos prédictions ($30\\%$). \n",
    "\n",
    "Nous utilisons la fonction <a href=\"http://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.randomSplit\"> `randomSplit` </a> qui divise aléatoirement un RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Total set]     57930 examples (2822 in class +1).\n",
      "[Training set]  40561 examples (2001 in class +1).\n",
      "[Testing set]   17369 examples (821 in class +1).\n"
     ]
    }
   ],
   "source": [
    "weights = [.7, .3]\n",
    "seed = 42\n",
    "\n",
    "trainRDD, testRDD = data_scaled.randomSplit(weights, seed)\n",
    "\n",
    "numExamplestrain = trainRDD.count()\n",
    "numClass1train   = trainRDD.map(lambda ex: ex.label==1.0).sum()\n",
    "\n",
    "numExamplestest = testRDD.count()\n",
    "numClass1test   = testRDD.map(lambda ex: ex.label==1.0).sum()\n",
    "\n",
    "print(\"[Total set]     %d examples (%d in class +1).\" % (numExamples, numClass1))\n",
    "print(\"[Training set]  %d examples (%d in class +1).\" % (numExamplestrain, numClass1train))\n",
    "print(\"[Testing set]   %d examples (%d in class +1).\" % (numExamplestest, numClass1test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(0.0, [-0.000414035031732,-0.0024740538601,-0.00341258853829,0.0124919073001,0.00520893750325,0.00738817026779,-0.00443563405927,0.0186510548985,-0.00103584854704,0.0131807431244,0.0109816170485,1.0])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pour arrêter Spark\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
